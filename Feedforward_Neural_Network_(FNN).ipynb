{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Initialization:\n",
        "\n",
        "The FeedForwardNN class initializes the weights and biases for each layer, scaled for small initial values. W1 and b1 are for the hidden layer, while W2 and b2 are for the output layer.\n",
        "The learning_rate controls how much we adjust the weights in each training step.\n",
        "Activation Functions:\n",
        "\n",
        "ReLU: The hidden layer uses a ReLU activation function. The ReLU derivative is also implemented for backpropagation.\n",
        "Softmax: The output layer uses softmax, suitable for multi-class classification, ensuring output probabilities sum up to 1.\n",
        "Forward Pass:\n",
        "\n",
        "forward(x): Computes the activations and final output by passing inputs through each layer.\n",
        "Z1 is the linear transformation of the input to the hidden layer, and A1 is the result after applying ReLU.\n",
        "Z2 and A2 represent the linear transformation and softmax output for the output layer.\n",
        "Backward Pass:\n",
        "\n",
        "backward(x, y): Computes gradients for weights and biases using backpropagation.\n",
        "Output Layer Gradients:\n",
        "dZ2 calculates the difference between predicted and actual output.\n",
        "dW2 and db2 update the weights and biases for the output layer.\n",
        "Hidden Layer Gradients:\n",
        "dA1 propagates the error to the hidden layer, and dZ1 adjusts based on the derivative of ReLU.\n",
        "dW1 and db1 update the hidden layerâ€™s weights and biases.\n",
        "Training:\n",
        "\n",
        "train(x, y, epochs): Iteratively calls forward and backward for multiple epochs, updating weights and biases to reduce the loss.\n",
        "Loss Calculation: We use cross-entropy loss for training, calculated by comparing predicted outputs with actual labels."
      ],
      "metadata": {
        "id": "LhitcoqIWNBC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54RiIjiXWLYi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class FeedForwardNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        # Initialize network architecture and learning rate\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.W1 = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden layer weights\n",
        "        self.b1 = np.zeros((hidden_size, 1))                       # Hidden layer bias\n",
        "        self.W2 = np.random.randn(output_size, hidden_size) * 0.01 # Hidden to output layer weights\n",
        "        self.b2 = np.zeros((output_size, 1))                       # Output layer bias\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        return np.where(x > 0, 1, 0)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exps = np.exp(x - np.max(x, axis=0, keepdims=True))  # Stability improvement by subtracting max\n",
        "        return exps / np.sum(exps, axis=0, keepdims=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass: compute activations and outputs\n",
        "        self.Z1 = np.dot(self.W1, x) + self.b1      # Linear transformation for hidden layer\n",
        "        self.A1 = self.relu(self.Z1)                # ReLU activation for hidden layer\n",
        "        self.Z2 = np.dot(self.W2, self.A1) + self.b2 # Linear transformation for output layer\n",
        "        self.A2 = self.softmax(self.Z2)             # Softmax for output probabilities\n",
        "\n",
        "        return self.A2\n",
        "\n",
        "    def backward(self, x, y):\n",
        "        # Compute gradients using backpropagation\n",
        "        m = y.shape[1]  # Batch size\n",
        "\n",
        "        # Output layer gradient\n",
        "        dZ2 = self.A2 - y\n",
        "        dW2 = (1 / m) * np.dot(dZ2, self.A1.T)\n",
        "        db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "        # Hidden layer gradient\n",
        "        dA1 = np.dot(self.W2.T, dZ2)\n",
        "        dZ1 = dA1 * self.relu_derivative(self.Z1)\n",
        "        dW1 = (1 / m) * np.dot(dZ1, x.T)\n",
        "        db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.W1 -= self.learning_rate * dW1\n",
        "        self.b1 -= self.learning_rate * db1\n",
        "        self.W2 -= self.learning_rate * dW2\n",
        "        self.b2 -= self.learning_rate * db2\n",
        "\n",
        "    def train(self, x, y, epochs=100):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            output = self.forward(x)\n",
        "\n",
        "            # Calculate loss (cross-entropy)\n",
        "            loss = -np.mean(np.sum(y * np.log(output), axis=0))\n",
        "\n",
        "            # Backward pass\n",
        "            self.backward(x, y)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Suppose we have a dataset with input features of size 3 and two classes to classify\n",
        "input_size = 3\n",
        "hidden_size = 5\n",
        "output_size = 2\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Randomly generated inputs (features) and one-hot encoded target labels\n",
        "x = np.random.randn(input_size, 10)  # 10 examples, each with 3 features\n",
        "y = np.zeros((output_size, 10))\n",
        "y[0, :5] = 1  # First 5 examples belong to class 0\n",
        "y[1, 5:] = 1  # Last 5 examples belong to class 1\n",
        "\n",
        "# Create and train the neural network\n",
        "nn = FeedForwardNN(input_size, hidden_size, output_size, learning_rate)\n",
        "nn.train(x, y, epochs=100)\n"
      ]
    }
  ]
}