{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Initialization: The RNN is initialized with random weights for each layer (input-to-hidden, hidden-to-hidden, hidden-to-output). Bias terms for the hidden and output layers are initialized to zero.\n",
        "\n",
        "Forward Pass:\n",
        "\n",
        "For each input in the sequence, we calculate the hidden state using the input-to-hidden weights, the previous hidden state, and the bias term.\n",
        "The output is calculated based on the current hidden state and the output weights.\n",
        "The hidden states and outputs are stored for use in backpropagation.\n",
        "Backward Pass (BPTT):\n",
        "\n",
        "We compute the gradient of the loss with respect to each weight and bias by iterating backward through the sequence (backpropagation through time).\n",
        "We update the weights using the calculated gradients and a learning rate.\n",
        "Training: The train function iterates through the forward and backward passes for a given number of epochs."
      ],
      "metadata": {
        "id": "LhitcoqIWNBC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54RiIjiXWLYi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class RNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        # Initialize dimensions\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize weights\n",
        "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden\n",
        "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden\n",
        "        self.Why = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output\n",
        "        self.bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
        "        self.by = np.zeros((output_size, 1))  # Output bias\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        h_prev = np.zeros((self.hidden_size, 1))\n",
        "        self.hidden_states = [h_prev]\n",
        "\n",
        "        # Forward pass\n",
        "        self.outputs = []\n",
        "        for x in inputs:\n",
        "            x = x.reshape(-1, 1)\n",
        "            h_prev = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h_prev) + self.bh)\n",
        "            y = np.dot(self.Why, h_prev) + self.by\n",
        "            self.outputs.append(y)\n",
        "            self.hidden_states.append(h_prev)\n",
        "\n",
        "        return self.outputs\n",
        "\n",
        "    def backward(self, inputs, targets):\n",
        "        dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
        "        dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
        "\n",
        "        # Initialize gradients for h_prev\n",
        "        dh_next = np.zeros_like(self.hidden_states[0])\n",
        "\n",
        "        # Backpropagation through time\n",
        "        for t in reversed(range(len(inputs))):\n",
        "            dy = self.outputs[t] - targets[t].reshape(-1, 1)\n",
        "            dWhy += np.dot(dy, self.hidden_states[t + 1].T)\n",
        "            dby += dy\n",
        "\n",
        "            dh = np.dot(self.Why.T, dy) + dh_next\n",
        "            dh_raw = (1 - self.hidden_states[t + 1] ** 2) * dh  # Derivative of tanh\n",
        "            dbh += dh_raw\n",
        "            dWxh += np.dot(dh_raw, inputs[t].reshape(1, -1))\n",
        "            dWhh += np.dot(dh_raw, self.hidden_states[t].T)\n",
        "            dh_next = np.dot(self.Whh.T, dh_raw)\n",
        "\n",
        "        # Update weights\n",
        "        for param, dparam in zip([self.Wxh, self.Whh, self.Why, self.bh, self.by],\n",
        "                                 [dWxh, dWhh, dWhy, dbh, dby]):\n",
        "            param -= self.learning_rate * dparam\n",
        "\n",
        "    def train(self, inputs, targets, epochs=100):\n",
        "        for epoch in range(epochs):\n",
        "            outputs = self.forward(inputs)\n",
        "            self.backward(inputs, targets)\n",
        "            if epoch % 10 == 0:\n",
        "                loss = sum((target.reshape(-1, 1) - output) ** 2 for target, output in zip(targets, outputs))\n",
        "                print(f'Epoch {epoch}, Loss: {np.sum(loss)}')\n",
        "\n",
        "# Sample input (for sequence length = 3, input size = 2)\n",
        "inputs = [np.array([0.5, -0.2]), np.array([0.1, 0.8]), np.array([-0.3, 0.2])]\n",
        "# Sample target output (for sequence length = 3, output size = 1)\n",
        "targets = [np.array([0.1]), np.array([0.4]), np.array([-0.1])]\n",
        "\n",
        "# Create and train RNN\n",
        "rnn = RNN(input_size=2, hidden_size=4, output_size=1, learning_rate=0.01)\n",
        "rnn.train(inputs, targets, epochs=100)"
      ]
    }
  ]
}