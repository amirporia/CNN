{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Positional Encoding: The positional_encoding function provides a unique encoding for each position in the sequence to retain order information in the input embeddings.\n",
        "\n",
        "Multi-Head Attention:\n",
        "\n",
        "MultiHeadAttention class performs self-attention across multiple heads.\n",
        "Each head computes scaled dot-product attention with different projections of the input.\n",
        "The outputs of all heads are concatenated and linearly transformed.\n",
        "Feed-Forward Network: The FeedForward class represents a two-layer fully connected network with a ReLU activation in the middle.\n",
        "\n",
        "Layer Normalization: The LayerNormalization class normalizes the output of each sub-layer.\n",
        "\n",
        "Transformer Model: The Transformer class integrates all components (embedding, attention, feed-forward, and normalization layers) to process the input through an encoder layer."
      ],
      "metadata": {
        "id": "LhitcoqIWNBC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54RiIjiXWLYi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Transformer:\n",
        "    def __init__(self, d_model, num_heads, d_ff, input_vocab_size, max_seq_len):\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_ff = d_ff\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        # Embedding and positional encoding\n",
        "        self.embedding = np.random.randn(input_vocab_size, d_model) * 0.01\n",
        "        self.pos_encoding = self.positional_encoding(max_seq_len, d_model)\n",
        "\n",
        "        # Initialize layers\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        self.layer_norm1 = LayerNormalization(d_model)\n",
        "        self.layer_norm2 = LayerNormalization(d_model)\n",
        "\n",
        "    def positional_encoding(self, max_len, d_model):\n",
        "        pos_encoding = np.zeros((max_len, d_model))\n",
        "        for pos in range(max_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pos_encoding[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\n",
        "                pos_encoding[pos, i + 1] = np.cos(pos / (10000 ** (i / d_model)))\n",
        "        return pos_encoding\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embedding and positional encoding\n",
        "        x = self.embedding[x] + self.pos_encoding[:x.shape[0]]\n",
        "\n",
        "        # Encoder layers\n",
        "        attn_out = self.attention(x, x, x)\n",
        "        x = self.layer_norm1(x + attn_out)\n",
        "\n",
        "        ff_out = self.feed_forward(x)\n",
        "        x = self.layer_norm2(x + ff_out)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention:\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        # Initialize weights\n",
        "        self.Wq = np.random.randn(d_model, d_model) * 0.01\n",
        "        self.Wk = np.random.randn(d_model, d_model) * 0.01\n",
        "        self.Wv = np.random.randn(d_model, d_model) * 0.01\n",
        "        self.Wo = np.random.randn(d_model, d_model) * 0.01\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Split into heads\n",
        "        x = x.reshape(x.shape[0], -1, self.num_heads, self.depth)\n",
        "        return x.transpose(1, 2, 0, 3)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V):\n",
        "        matmul_qk = np.matmul(Q, K.transpose(0, 1, 3, 2))\n",
        "        dk = K.shape[-1]\n",
        "        scaled_attention_logits = matmul_qk / np.sqrt(dk)\n",
        "        attention_weights = np.exp(scaled_attention_logits)\n",
        "        attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n",
        "        output = np.matmul(attention_weights, V)\n",
        "        return output\n",
        "\n",
        "    def forward(self, Q, K, V):\n",
        "        # Linear projections\n",
        "        Q = np.dot(Q, self.Wq)\n",
        "        K = np.dot(K, self.Wk)\n",
        "        V = np.dot(V, self.Wv)\n",
        "\n",
        "        # Split heads\n",
        "        Q = self.split_heads(Q)\n",
        "        K = self.split_heads(K)\n",
        "        V = self.split_heads(V)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attention = self.scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "        # Concatenate heads\n",
        "        attention = attention.transpose(2, 0, 1, 3).reshape(Q.shape[2], -1, self.d_model)\n",
        "\n",
        "        # Final linear layer\n",
        "        output = np.dot(attention, self.Wo)\n",
        "        return output\n",
        "\n",
        "\n",
        "class FeedForward:\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        # Initialize weights\n",
        "        self.W1 = np.random.randn(d_model, d_ff) * 0.01\n",
        "        self.W2 = np.random.randn(d_ff, d_model) * 0.01\n",
        "        self.b1 = np.zeros((1, d_ff))\n",
        "        self.b2 = np.zeros((1, d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = np.dot(x, self.W1) + self.b1\n",
        "        x = np.maximum(0, x)  # ReLU activation\n",
        "        x = np.dot(x, self.W2) + self.b2\n",
        "        return x\n",
        "\n",
        "\n",
        "class LayerNormalization:\n",
        "    def __init__(self, d_model, epsilon=1e-6):\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = np.ones((1, d_model))\n",
        "        self.beta = np.zeros((1, d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = np.mean(x, axis=-1, keepdims=True)\n",
        "        var = np.var(x, axis=-1, keepdims=True)\n",
        "        x_normalized = (x - mean) / np.sqrt(var + self.epsilon)\n",
        "        return self.gamma * x_normalized + self.beta\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# Input configuration\n",
        "input_vocab_size = 1000\n",
        "max_seq_len = 10\n",
        "d_model = 64\n",
        "num_heads = 4\n",
        "d_ff = 128\n",
        "\n",
        "# Example sequence (length = 10) with random token indices\n",
        "x = np.random.randint(0, input_vocab_size, size=(max_seq_len,))\n",
        "\n",
        "# Initialize Transformer model and pass input through it\n",
        "transformer = Transformer(d_model=d_model, num_heads=num_heads, d_ff=d_ff,\n",
        "                          input_vocab_size=input_vocab_size, max_seq_len=max_seq_len)\n",
        "output = transformer.forward(x)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n"
      ]
    }
  ]
}